{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNfT0Oi5sT2ZzSa/Hepg8zu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Manvi190502/AgenticAI/blob/main/RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VMqEUyfQTenL"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip install langchain langchain-core langchain_community langgraph langchain-huggingface transformers torch langchain_chroma unstructured"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install unstructured\n",
        "from logging import LoggerAdapter\n",
        "from langchain_community.document_loaders import UnstructuredURLLoader\n",
        "\n",
        "urls=['https://docs.langchain.com/oss/python/langgraph/overview']\n",
        "Loader = UnstructuredURLLoader(urls=urls)\n",
        "data = Loader.load()"
      ],
      "metadata": {
        "id": "fxFNXFkwTuBF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "3rJcqNrcU7FK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "all_splits = text_splitter.split_documents(data)\n",
        "print(\"Total number of documents: \",len(all_splits))"
      ],
      "metadata": {
        "id": "OG0S0nBhVS_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "embeddings = HuggingFaceEmbeddings()\n",
        "\n",
        "vector = embeddings.embed_query(\"hello, world!\")\n",
        "vector[:5]"
      ],
      "metadata": {
        "id": "80S5f3pvWFEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_chroma import Chroma\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "vectorstore = Chroma.from_documents(documents=all_splits, embedding=HuggingFaceEmbeddings())"
      ],
      "metadata": {
        "id": "PVegSHwjXKaI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#if you want to store chromaDb locally\n",
        "'''\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=all_splits,\n",
        "    embedding=HuggingFaceEmbeddings(),\n",
        "    persist_directory=\"./chroma_db\"    #Custom directory\n",
        ")\n",
        "'''\n",
        "\n",
        "#loading the database later\n",
        "'''\n",
        "vectorstore = Chroma(\n",
        "    persist_directory=\"./chroma_db\",\n",
        "    embedding_function=HuggingFaceEmbeddings(),\n",
        "'''"
      ],
      "metadata": {
        "id": "vWSVf414Xies"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "import torch\n",
        "\n",
        "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "text_generation_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=512,\n",
        "    temperature=0.7,\n",
        ")\n",
        "\n",
        "from langchain_huggingface import HuggingFacePipeline\n",
        "llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n"
      ],
      "metadata": {
        "id": "9nmx6JRNYnwZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "You are a helpful assistant.\n",
        "Answer the question using ONLY the provided context.\n",
        "If the answer is not in the context, say:\n",
        "\"I don't know based on the provided information.\"\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Answer:\n",
        "\"\"\")\n"
      ],
      "metadata": {
        "id": "7KopqxWcaI5e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing_extensions import List, TypedDict\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "class State(TypedDict):\n",
        "  question: str\n",
        "  context: List[Document]\n",
        "  answer: str"
      ],
      "metadata": {
        "id": "GF4opUjSav30"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve(state: State):\n",
        "  retrieved_docs = vectorstore.similarity_search(state[\"question\"],k=1)\n",
        "  return {\"context\": retrieved_docs}"
      ],
      "metadata": {
        "collapsed": true,
        "id": "C0IpyDwvbACp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(state: State):\n",
        "  docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
        "  messages = prompt.invoke({\"question\":state[\"question\"],\"context\":docs_content})\n",
        "  response = llm.invoke(messages)\n",
        "  return {\"answer\": response}"
      ],
      "metadata": {
        "collapsed": true,
        "id": "qJpCAUcTba_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import START, StateGraph\n",
        "graph_builder= StateGraph(State).add_sequence([retrieve, generate])\n",
        "graph_builder.add_edge(START,\"retrieve\")\n",
        "graph = graph_builder.compile()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "4hXOpbXYcQdZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Image\n",
        "display(Image(graph.get_graph().draw_mermaid_png()))"
      ],
      "metadata": {
        "collapsed": true,
        "id": "3Nqr75Q7dPoe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = graph.invoke({\"question\":\"What is langgraph?\"})\n",
        "print(response[\"answer\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "o2XS3zqRd2ud",
        "outputId": "32dc2bb0-c37e-4e9d-c294-8601e5ffe2bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: \n",
            "You are a helpful assistant.\n",
            "Answer the question using ONLY the provided context.\n",
            "If the answer is not in the context, say:\n",
            "\"I don't know based on the provided information.\"\n",
            "\n",
            "Context:\n",
            "Skip to main content\n",
            "\n",
            "Docs by LangChain home page\n",
            "\n",
            "light logo\n",
            "\n",
            "dark logo\n",
            "\n",
            "Deep AgentsLangChainLangGraphIntegrationsLearnReferenceContribute\n",
            "\n",
            "Overview\n",
            "\n",
            "Get started\n",
            "\n",
            "Install\n",
            "\n",
            "Quickstart\n",
            "\n",
            "Local server\n",
            "\n",
            "Changelog\n",
            "\n",
            "Thinking in LangGraph\n",
            "\n",
            "Workflows + agents\n",
            "\n",
            "Capabilities\n",
            "\n",
            "Persistence\n",
            "\n",
            "Durable execution\n",
            "\n",
            "Streaming\n",
            "\n",
            "Interrupts\n",
            "\n",
            "Time travel\n",
            "\n",
            "Memory\n",
            "\n",
            "Subgraphs\n",
            "\n",
            "Production\n",
            "\n",
            "Application structure\n",
            "\n",
            "Test\n",
            "\n",
            "LangSmith Studio\n",
            "\n",
            "Agent Chat UI\n",
            "\n",
            "LangSmith Deployment\n",
            "\n",
            "LangSmith Observability\n",
            "\n",
            "LangGraph APIs\n",
            "\n",
            "Runtime\n",
            "\n",
            "Install\n",
            "\n",
            "Core benefits\n",
            "\n",
            "LangGraph ecosystem\n",
            "\n",
            "Acknowledgements\n",
            "\n",
            "LangGraph overview\n",
            "\n",
            "Gain control with LangGraph to design agents that reliably handle complex tasks\n",
            "\n",
            "Question:\n",
            "What is langgraph?\n",
            "\n",
            "Answer:\n",
            "Langgraph is an open-source tool for building and deploying agents that execute code in a distributed and resilient way. It supports a variety of programming languages and integrates with a wide range of frameworks and libraries. It provides a high degree of control over the structure and behavior of agents, making it ideal for building systems that handle complex tasks and interactions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Dg9th89ZeB0Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}